{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the analysis for the ConvX (a Udacity Capstone Project). Before starting, be sure to follow the instructions in the README for the project on github (https://github.com/justiniann/ConvX).\n",
    "\n",
    "This code was originally run on a personal desktop computer. The specs were...\n",
    "\n",
    "CPU: Intel i5 6600K, \n",
    "GPU: Nvidia 1060 GTX, \n",
    "RAM: 16GB\n",
    "\n",
    "I highly recommend that anyone attempting to run this code on the full dataset use hardware that is comparable or better.\n",
    "\n",
    "First, we need to load our base model. We will also define a few variables that we will need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from keras.applications import ResNet50\n",
    "from keras.layers import Dropout, GlobalAveragePooling2D, Dense, Flatten\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import model_from_json\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, fbeta_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from convx_utils import *\n",
    "\n",
    "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(512, 512, 3))\n",
    "target_image_size = (512, 512)\n",
    "batch_size = 32\n",
    "transfer_learning_epochs = 500\n",
    "fine_tuning_epochs = 5\n",
    "fine_tuning_layers_to_train = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define some paths that we will need later for saving various results as we run through our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following are directories used for reading/saving data\n",
    "RES_PATH = \"..{0}..{0}resources{0}\".format(os.path.sep)\n",
    "IMG_PATH = \"..{0}..{0}images{0}\".format(os.path.sep)\n",
    "BOTTLENECK_PATH = \"..{0}..{0}bottleneck{0}\".format(os.path.sep)\n",
    "SAVE_PATH = \"..{0}..{0}saved_models{0}\".format(os.path.sep)\n",
    "TRAIN_PATH = os.path.join(IMG_PATH, \"train\")\n",
    "VAL_PATH = os.path.join(IMG_PATH, \"validation\")\n",
    "TEST_PATH = os.path.join(IMG_PATH, \"test\")\n",
    "\n",
    "model_name = \"convx_model\"  # The directory all data will be saved in will be named whatever this value is.\n",
    "models_save_directory = os.path.join(SAVE_PATH, model_name)\n",
    "build_dir_path(models_save_directory)  # build the directory structure we need for saving results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't included the top layer because we are going to build and train the top layer ourselves. This is known as transfer learning, and it is the first major step in training our model.\n",
    "\n",
    "Before starting that, however, we need to get a few variables we are going to need durring processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_files(root_dir):\n",
    "    return sum([len(files) for r, d, files in os.walk(root_dir)])\n",
    "\n",
    "def get_iterations_per_epoch(total_images, batch_size):\n",
    "    return np.ceil(total_images / batch_size)\n",
    "\n",
    "healthy_train_images = count_files(os.path.join(TRAIN_PATH, \"healthy\"))\n",
    "unhealthy_train_images = count_files(os.path.join(TRAIN_PATH, \"unhealthy\"))\n",
    "healthy_validation_images = count_files(os.path.join(VAL_PATH, \"healthy\"))\n",
    "unhealthy_validation_images = count_files(os.path.join(VAL_PATH, \"unhealthy\"))\n",
    "\n",
    "num_training_steps = get_iterations_per_epoch((healthy_train_images + unhealthy_train_images), batch_size)\n",
    "num_validation_steps = get_iterations_per_epoch((healthy_validation_images + unhealthy_validation_images), batch_size)\n",
    "\n",
    "data_generator = ImageDataGenerator(\n",
    "    rescale=1. / 255\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For efficiency, we are going to get and save the bottleneck features for this model before we start with the transfer learning. By obtaining and saving these once, we can avoid having to run every image through the entire network durring every epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_file_path = os.path.join(BOTTLENECK_PATH, model_name)\n",
    "train_bottleneck_file = os.path.join(bottleneck_file_path, \"train.npy\")\n",
    "validation_bottleneck_file = os.path.join(bottleneck_file_path, \"validation.npy\")\n",
    "    \n",
    "# Extract bottleneck features if they have not been already\n",
    "if not os.path.exists(bottleneck_file_path):\n",
    "    train_generator = data_generator.flow_from_directory(\n",
    "        TRAIN_PATH,\n",
    "        target_size=target_image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    bottleneck_features_train = base_model.predict_generator(train_generator, num_training_steps)\n",
    "    build_dir_path(bottleneck_file_path)\n",
    "    np.save(open(train_bottleneck_file, 'wb'), bottleneck_features_train)\n",
    "    \n",
    "    validation_path_generator = data_generator.flow_from_directory(\n",
    "        VAL_PATH,\n",
    "        target_size=target_image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    bottleneck_features_validation = base_model.predict_generator(validation_path_generator, num_validation_steps)\n",
    "    np.save(open(validation_bottleneck_file, 'wb'), bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the bottleneck features established, we can start transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fully_connected_top_layer(connecting_shape):\n",
    "    top_layers = Sequential()\n",
    "    top_layers.add(GlobalAveragePooling2D(input_shape=connecting_shape))\n",
    "    top_layers.add(Dense(512, activation='tanh'))\n",
    "    top_layers.add(Dropout(0.4))\n",
    "    top_layers.add(Dense(2, activation='softmax'))\n",
    "    return top_layers\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# load training data\n",
    "train_data = np.load(open(train_bottleneck_file, 'rb'))\n",
    "train_labels = to_categorical(np.array(([0] * healthy_train_images) + ([1] * unhealthy_train_images)),\n",
    "                              num_classes=2)\n",
    "# load validation data\n",
    "validation_data = np.load(open(validation_bottleneck_file, 'rb'))\n",
    "validation_labels = to_categorical(np.array([0] * healthy_validation_images + [1] * unhealthy_validation_images),\n",
    "                                   num_classes=2)\n",
    "\n",
    "top_layer = build_fully_connected_top_layer(train_data.shape[1:])\n",
    "\n",
    "compile_model(top_layer)\n",
    "\n",
    "transfer_history = top_layer.fit(train_data, train_labels,\n",
    "                                  epochs=transfer_learning_epochs,\n",
    "                                  batch_size=batch_size,\n",
    "                                  validation_data=(validation_data, validation_labels),\n",
    "                                  shuffle=True,\n",
    "                                  verbose=1)\n",
    "\n",
    "top_layers_weights_path = os.path.join(models_save_directory, \"transfer_learning_weights_v2.h5\")\n",
    "top_layer.save_weights(top_layers_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning has been completed and the results have been saved! We can now combine the base model with our newly trained top layer and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display_history(history):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model Binary Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0.4,0.8)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xlim(0,500)\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Binary Cross-Entropy Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0.6,0.8)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xlim(0,500)\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "display_history(transfer_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further improve our results by finetuning the model. Using the transfer learning model that we have already trained, we can 'unfreeze' a few of the layers from the base model. This will allow them to be trained, giving us an even better fit on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_layer = build_fully_connected_top_layer(base_model.output_shape[1:])\n",
    "top_layer.load_weights(top_layers_weights_path)\n",
    "convx_model = Model(inputs=base_model.input, outputs=top_layer(base_model.output))\n",
    "\n",
    "for layer in convx_model.layers[:len(convx_model.layers) - fine_tuning_layers_to_train]:\n",
    "    layer.trainable = False\n",
    "\n",
    "compile_model(convx_model)\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    target_size=target_image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    VAL_PATH,\n",
    "    target_size=target_image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "fine_tune_history = convx_model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=num_training_steps,\n",
    "    epochs=fine_tuning_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=num_validation_steps,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "convx_model.save_weights(os.path.join(models_save_directory, \"best_model_weights_v2.h5\"))\n",
    "\n",
    "with open(os.path.join(models_save_directory, \"best_model_v2.json\".format(model_name)), \"w\") as json_file:\n",
    "    json_file.write(convx_model.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we will plot the history of our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_history(fine_tune_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has now been fine tuned and the training process is complete! Lets evaluate the results. While we are also going to look at accuracy, our primary metric of evaluation is going to be f-beta, with a beta score of three. With this, we will get to see which models fit the data well while giving more weight to good recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convx_model = None\n",
    "with open(os.path.join(models_save_directory, \"best_model.json\"), 'r') as model_file:\n",
    "    convx_model = model_from_json(model_file.read())\n",
    "    \n",
    "convx_model.load_weights(os.path.join(models_save_directory, \"best_model_weights.h5\"))\n",
    "compile_model(convx_model)\n",
    "\n",
    "healthy_test_images = count_files(os.path.join(TEST_PATH, \"healthy\"))\n",
    "unhealthy_test_images = count_files(os.path.join(TEST_PATH, \"unhealthy\"))\n",
    "test_iteration_count = get_iterations_per_epoch((healthy_test_images + unhealthy_test_images), batch_size)\n",
    "\n",
    "# load test data\n",
    "test_labels = np.array(([0] * healthy_test_images) + ([1] * unhealthy_test_images))\n",
    "formatted_test_labels = to_categorical(test_labels, num_classes=2)\n",
    "\n",
    "test_generator = data_generator.flow_from_directory(\n",
    "    TEST_PATH,\n",
    "    target_size=target_image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "raw_predictions = convx_model.predict_generator(test_generator, test_iteration_count, verbose=1)\n",
    "predicted_labels = np.argmax(raw_predictions, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "\n",
    "f3_score = fbeta_score(test_labels, predicted_labels, 3)\n",
    "print(\"F3 Score: {}\".format(f3_score))\n",
    "\n",
    "conf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
    "print(\"Confusion Matrix: \\n{}\".format(conf_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we give more weight to good recall over good precision, the numbers are much different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_weight(raw_predictions, p):\n",
    "    res = []\n",
    "    for pred in raw_predictions:\n",
    "        res.append(0 if pred[0] > p else 1)\n",
    "    return res\n",
    "\n",
    "x = []\n",
    "true_negatives = []\n",
    "false_negatives = []\n",
    "for i in range(50, 100):\n",
    "    weight = i/100\n",
    "    x.append(weight)\n",
    "    weighted_predictions = predict_with_weight(raw_predictions, weight)\n",
    "    conf_matrix = confusion_matrix(test_labels, weighted_predictions)\n",
    "    true_negatives.append(conf_matrix[0][0])\n",
    "    false_negatives.append(conf_matrix[1][0])\n",
    "    \n",
    "plt.plot(x, true_negatives, 'b')\n",
    "plt.plot(x, false_negatives, 'r')\n",
    "plt.title('True Negatives vs False Negatives')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Unhealthy Class Weight')\n",
    "plt.legend(['True Negatives', 'False Negatives'], loc='upper right')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
