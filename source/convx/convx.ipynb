{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the analysis for the ConvX (a Udacity Capstone Project). Before starting, be sure to follow the instructions in the README for the project on github (https://github.com/justiniann/ConvX).\n",
    "\n",
    "This code was originally run on a personal desktop computer. The specs were...\n",
    "\n",
    "CPU: Intel i5 6600K, \n",
    "GPU: Nvidia 1060 GTX, \n",
    "RAM: 16GB\n",
    "\n",
    "I highly recommend that anyone attempting to run this code on the full dataset use hardware that is comparable or better.\n",
    "\n",
    "First, we need to load our base model. We will also define a few variables that we will need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from keras.applications import ResNet50\n",
    "from keras.layers import Dropout, GlobalAveragePooling2D, Dense\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import model_from_json\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, fbeta_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from convx_utils import *\n",
    "\n",
    "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(512, 512, 3))\n",
    "target_image_size = (512, 512)\n",
    "batch_size = 32\n",
    "transfer_learning_epochs = 1000\n",
    "fine_tuning_epochs = 5\n",
    "fine_tuning_layers_to_train = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define some paths that we will need later for saving various results as we run through our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following are directories used for reading/saving data\n",
    "RES_PATH = \"..{0}..{0}resources{0}\".format(os.path.sep)\n",
    "IMG_PATH = \"..{0}..{0}images{0}\".format(os.path.sep)\n",
    "BOTTLENECK_PATH = \"..{0}..{0}bottleneck{0}\".format(os.path.sep)\n",
    "SAVE_PATH = \"..{0}..{0}saved_models{0}\".format(os.path.sep)\n",
    "TRAIN_PATH = os.path.join(IMG_PATH, \"train\")\n",
    "VAL_PATH = os.path.join(IMG_PATH, \"validation\")\n",
    "TEST_PATH = os.path.join(IMG_PATH, \"test\")\n",
    "\n",
    "model_name = \"convx_model\"  # The directory all data will be saved in will be named whatever this value is.\n",
    "models_save_directory = os.path.join(SAVE_PATH, model_name)\n",
    "build_dir_path(models_save_directory)  # build the directory structure we need for saving results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't included the top layer because we are going to build and train the top layer ourselves. This is known as transfer learning, and it is the first major step in training our model.\n",
    "\n",
    "Before starting that, however, we need to get a few variables we are going to need durring processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_files(root_dir):\n",
    "    return sum([len(files) for r, d, files in os.walk(root_dir)])\n",
    "\n",
    "def get_iterations_per_epoch(total_images, batch_size):\n",
    "    return np.ceil(total_images / batch_size)\n",
    "\n",
    "healthy_train_images = count_files(os.path.join(TRAIN_PATH, \"healthy\"))\n",
    "unhealthy_train_images = count_files(os.path.join(TRAIN_PATH, \"unhealthy\"))\n",
    "healthy_validation_images = count_files(os.path.join(VAL_PATH, \"healthy\"))\n",
    "unhealthy_validation_images = count_files(os.path.join(VAL_PATH, \"unhealthy\"))\n",
    "\n",
    "num_training_steps = get_iterations_per_epoch((healthy_train_images + unhealthy_train_images), batch_size)\n",
    "num_validation_steps = get_iterations_per_epoch((healthy_validation_images + unhealthy_validation_images), batch_size)\n",
    "\n",
    "data_generator = ImageDataGenerator(\n",
    "    rescale=1. / 255\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For efficiency, we are going to get and save the bottleneck features for this model before we start with the transfer learning. By obtaining and saving these once, we can avoid having to run every image through the entire network durring every epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_file_path = os.path.join(BOTTLENECK_PATH, model_name)\n",
    "train_bottleneck_file = os.path.join(bottleneck_file_path, \"train.npy\")\n",
    "validation_bottleneck_file = os.path.join(bottleneck_file_path, \"validation.npy\")\n",
    "    \n",
    "# Extract bottleneck features if they have not been already\n",
    "if not os.path.exists(bottleneck_file_path):\n",
    "    train_generator = data_generator.flow_from_directory(\n",
    "        TRAIN_PATH,\n",
    "        target_size=target_image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    bottleneck_features_train = base_model.predict_generator(train_generator, num_training_steps)\n",
    "    build_dir_path(bottleneck_file_path)\n",
    "    np.save(open(train_bottleneck_file, 'wb'), bottleneck_features_train)\n",
    "    \n",
    "    validation_path_generator = data_generator.flow_from_directory(\n",
    "        VAL_PATH,\n",
    "        target_size=target_image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    bottleneck_features_validation = base_model.predict_generator(validation_path_generator, num_validation_steps)\n",
    "    np.save(open(validation_bottleneck_file, 'wb'), bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the bottleneck features established, we can start transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80725 samples, validate on 8970 samples\n",
      "Epoch 1/1000\n",
      "80725/80725 [==============================] - 13s - loss: 7.4022 - acc: 0.5380 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 2/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 3/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 4/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 5/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 6/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 7/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 8/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 9/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 10/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 11/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 12/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 13/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 14/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 15/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 16/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 17/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 18/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 19/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 20/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 21/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 22/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 23/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 24/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 25/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 26/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 27/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 28/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 29/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 30/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 31/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 32/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 33/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 34/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 35/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 36/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 37/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 38/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 39/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 40/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 41/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 42/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 43/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 44/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 45/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 46/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 47/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 48/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 49/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 50/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 51/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 52/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 53/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 54/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 55/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 56/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 57/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 58/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 59/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 60/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 61/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 62/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 63/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 64/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 65/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 66/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 67/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 68/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 69/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 70/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 71/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 72/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 73/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 74/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 75/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 76/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 77/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 78/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 79/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 80/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 81/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 82/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 83/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 84/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 85/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 86/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 87/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 88/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 89/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 90/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 91/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 92/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 93/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 94/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 95/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 96/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 97/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 98/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 99/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 100/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 101/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 102/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 103/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 104/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 105/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 106/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 107/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 108/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 109/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 110/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 111/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 112/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 113/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 114/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 115/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 116/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 117/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 118/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 119/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 120/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 121/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 122/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 123/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 124/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 125/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 126/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 127/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 128/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 129/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 130/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 131/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 132/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 133/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 134/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 135/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 136/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 137/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 138/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 139/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 140/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 141/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 142/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 143/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 144/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 145/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 146/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 147/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 148/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 149/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 150/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 151/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 152/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 153/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 154/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 155/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 156/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 157/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 158/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 159/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 160/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 161/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 162/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 163/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 164/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 165/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 166/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 167/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 168/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 169/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 170/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 171/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 172/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 173/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 174/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 175/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 176/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 177/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 178/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 179/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 180/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 181/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 182/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 183/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 184/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 185/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 186/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 187/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 188/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 189/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 190/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 191/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 192/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 193/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 194/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 195/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 196/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 197/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 198/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 199/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 200/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 201/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 202/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 203/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 204/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 205/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 206/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 207/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 208/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 209/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 210/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 211/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 212/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 213/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 214/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 215/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 216/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 217/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 218/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 219/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 220/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 221/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 222/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 223/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 224/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 225/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 226/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 227/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 228/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 229/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 230/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 231/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 232/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 233/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 234/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 235/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 236/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 237/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 238/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 239/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 240/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 241/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 242/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 243/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 244/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 245/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 246/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 247/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 248/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 249/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 250/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 251/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 252/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 253/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 254/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 255/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 256/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 257/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 258/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 259/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 260/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 261/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 262/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 263/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 264/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 265/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 266/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 267/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 268/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 269/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 270/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 271/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 272/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 273/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 274/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 275/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 276/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 277/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 278/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 279/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 280/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 281/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 282/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 283/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 284/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 285/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 286/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 287/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 288/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 289/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 290/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 291/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 292/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 293/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 294/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 295/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 296/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 297/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 298/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 299/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 300/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 301/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 302/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 303/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 304/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 305/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 306/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 307/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 308/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 309/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 310/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 311/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 312/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 313/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 314/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 315/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 316/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 317/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 318/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 319/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 320/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 321/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 322/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 323/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 324/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 325/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 326/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 327/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 328/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 329/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 330/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 331/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 332/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 333/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 334/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 335/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 336/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 337/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 338/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 339/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 340/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 341/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 342/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 343/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 344/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 345/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 346/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 347/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 348/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 349/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 350/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 351/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 352/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 353/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 354/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 355/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 356/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 357/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 358/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 359/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 360/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 361/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 362/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 363/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 364/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 365/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 366/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 367/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 368/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 369/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 370/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 371/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 372/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 373/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 374/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 375/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 376/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 377/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 378/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 379/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 380/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 381/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 382/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 383/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 384/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 385/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 386/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 387/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 388/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 389/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 390/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 391/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 392/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 393/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 394/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 395/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 396/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 397/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 398/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 399/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 400/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 401/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 402/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 403/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 404/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 405/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 406/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 407/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 408/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 409/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 410/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 411/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 412/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 413/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 414/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 415/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 416/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 417/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 418/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 419/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 420/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 421/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 422/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 423/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 424/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 425/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 426/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 427/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 428/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 429/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 430/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 431/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 432/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 433/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 434/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 435/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 436/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 437/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 438/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 439/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 440/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 441/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 442/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 443/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 444/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 445/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 446/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 447/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 448/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 449/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 450/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 451/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 452/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 453/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 454/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 455/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 456/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 457/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 458/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 459/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 460/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 461/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 462/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 463/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 464/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 465/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 466/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 467/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 468/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 469/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 470/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 471/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 472/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 473/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 474/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 475/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 476/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 477/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 478/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 479/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 480/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 481/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 482/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 483/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 484/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 485/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 486/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 487/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 488/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 489/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 490/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 491/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 492/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 493/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 494/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 495/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 496/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 497/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 498/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 499/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 500/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 501/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 502/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 503/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 504/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 505/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 506/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 507/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 508/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 509/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 510/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 511/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 512/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 513/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 514/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 515/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 516/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 517/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 518/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 519/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 520/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 521/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 522/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 523/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 524/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 525/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 526/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 527/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 528/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 529/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 530/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 531/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 532/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 533/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 534/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 535/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 536/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 537/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 538/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 539/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 540/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 541/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 542/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 543/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 544/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 545/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 546/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 547/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 548/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 549/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 550/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 551/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 552/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 553/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 554/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 555/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 556/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 557/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 558/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 559/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 560/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 561/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 562/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 563/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 564/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 565/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 566/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 567/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 568/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 569/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 570/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 571/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 572/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 573/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 574/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 575/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 576/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 577/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 578/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 579/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 580/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 581/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 582/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 583/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 584/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 585/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 586/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 587/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 588/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 589/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 590/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 591/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 592/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 593/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 594/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 595/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 596/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 597/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 598/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 599/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 600/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 601/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 602/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 603/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 604/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 605/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 606/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 607/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 608/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 609/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 610/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 611/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 612/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 613/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 614/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 615/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 616/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 617/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 618/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 619/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 620/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 621/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 622/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 623/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 624/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 625/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 626/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 627/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 628/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 629/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 630/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 631/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 632/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 633/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 634/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 635/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 636/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 637/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 638/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 639/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 640/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 641/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 642/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 643/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 644/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 645/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 646/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 647/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 648/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 649/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 650/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 651/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 652/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 653/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 654/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 655/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 656/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 657/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 658/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 659/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 660/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 661/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 662/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 663/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 664/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 665/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 666/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 667/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 668/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 669/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 670/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 671/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 672/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 673/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 674/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 675/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 676/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 677/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 678/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 679/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 680/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 681/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 682/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 683/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 684/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 685/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 686/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 687/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 688/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 689/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 690/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 691/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 692/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 693/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 694/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 695/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 696/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 697/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 698/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 699/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 700/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 701/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 702/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 703/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 704/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 705/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 706/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 707/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 708/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 709/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 710/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 711/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 712/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 713/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 714/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 715/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 716/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 717/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 718/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 719/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 720/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 721/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 722/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 723/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 724/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 725/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 726/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 727/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 728/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 729/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 730/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 731/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 732/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 733/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 734/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 735/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 736/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 737/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 738/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 739/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 740/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 741/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 742/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 743/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 744/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 745/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 746/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 747/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 748/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 749/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 750/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 751/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 752/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 753/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 754/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 755/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 756/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 757/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 758/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 759/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 760/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 761/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 762/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 763/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 764/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 765/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 766/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 767/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 768/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 769/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 770/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 771/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 772/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 773/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 774/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 775/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 776/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 777/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 778/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 779/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 780/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 781/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 782/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 783/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 784/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 785/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 786/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 787/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 788/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 789/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 790/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 791/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 792/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 793/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 794/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 795/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 796/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 797/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 798/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 799/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 800/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 801/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 802/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 803/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 804/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 805/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 806/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 807/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 808/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 809/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 810/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 811/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 812/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 813/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 814/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 815/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 816/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 817/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 818/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 819/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 820/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 821/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 822/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 823/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 824/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 825/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 826/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 827/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 828/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 829/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 830/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 831/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 832/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 833/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 834/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 835/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 836/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 837/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 838/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 839/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 840/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 841/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 842/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 843/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 844/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 845/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 846/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 847/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 848/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 849/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 850/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 851/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 852/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 853/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 854/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 855/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 856/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 857/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 858/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 859/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 860/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 861/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 862/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 863/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 864/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 865/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 866/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 867/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 868/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 869/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 870/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 871/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 872/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 873/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 874/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 875/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 876/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 877/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 878/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 879/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 880/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 881/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 882/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 883/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 884/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 885/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 886/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 887/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 888/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 889/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 890/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 891/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 892/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 893/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 894/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 895/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 896/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 897/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 898/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 899/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 900/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 901/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 902/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 903/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 904/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 905/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 906/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 907/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 908/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 909/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 910/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 911/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 912/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 913/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 914/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 915/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 916/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 917/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 918/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 919/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 920/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 921/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 922/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 923/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 924/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 925/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 926/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 927/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 928/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 929/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 930/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 931/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 932/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 933/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 934/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 935/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 936/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 937/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 938/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 939/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 940/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 941/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 942/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 943/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 944/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 945/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 946/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 947/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 948/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 949/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 950/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 951/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 952/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 953/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 954/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 955/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 956/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 957/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 958/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 959/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 960/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 961/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 962/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 963/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 964/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 965/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 966/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 967/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 968/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 969/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 970/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 971/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 972/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 973/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 974/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 975/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 976/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 977/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 978/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 979/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 980/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 981/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 982/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 983/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 984/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 985/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 986/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 987/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 988/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 989/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 990/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 991/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 992/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 993/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 994/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 995/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 996/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 997/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 998/1000\n",
      "80725/80725 [==============================] - 12s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 999/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n",
      "Epoch 1000/1000\n",
      "80725/80725 [==============================] - 11s - loss: 7.4030 - acc: 0.5382 - val_loss: 7.4790 - val_acc: 0.5334\n"
     ]
    }
   ],
   "source": [
    "def build_fully_connected_top_layer(connecting_shape, dropout=True):\n",
    "    top_layers = Sequential()\n",
    "    top_layers.add(GlobalAveragePooling2D(input_shape=connecting_shape))\n",
    "    top_layers.add(Dense(512, activation='relu'))\n",
    "    if dropout:\n",
    "        top_layers.add(Dropout(0.2))\n",
    "    top_layers.add(Dense(2, activation='softmax'))\n",
    "    return top_layers\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# load training data\n",
    "train_data = np.load(open(train_bottleneck_file, 'rb'))\n",
    "train_labels = to_categorical(np.array(([0] * healthy_train_images) + ([1] * unhealthy_train_images)),\n",
    "                              num_classes=2)\n",
    "# load validation data\n",
    "validation_data = np.load(open(validation_bottleneck_file, 'rb'))\n",
    "validation_labels = to_categorical(np.array([0] * healthy_validation_images + [1] * unhealthy_validation_images),\n",
    "                                   num_classes=2)\n",
    "\n",
    "top_layer = build_fully_connected_top_layer(train_data.shape[1:])\n",
    "\n",
    "compile_model(top_layer)\n",
    "\n",
    "transfer_history = top_layer.fit(train_data, train_labels,\n",
    "                                  epochs=transfer_learning_epochs,\n",
    "                                  batch_size=batch_size,\n",
    "                                  validation_data=(validation_data, validation_labels),\n",
    "                                  shuffle=True,\n",
    "                                  verbose=1)\n",
    "\n",
    "top_layers_weights_path = os.path.join(models_save_directory, \"transfer_learning_weights.h5\")\n",
    "top_layer.save_weights(top_layers_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning has been completed and the results have been saved! We can now combine the base model with our newly trained top layer and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHl6XvgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17d6e319400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFXKyMDOzQk4WZmZW6P8DzS9ce7MzkMIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17d6fa39fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_history(history):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model Binary Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Binary Cross-Entropy Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "display_history(transfer_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further improve our results by finetuning the model. Using the transfer learning model that we have already trained, we can 'unfreeze' a few of the layers from the base model. This will allow them to be trained, giving us an even better fit on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80725 images belonging to 2 classes.\n",
      "Found 8970 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "2523/2523 [==============================] - 3610s - loss: 0.6212 - acc: 0.6646 - val_loss: 0.6117 - val_acc: 0.6775\n",
      "Epoch 2/5\n",
      "2523/2523 [==============================] - 3590s - loss: 0.5999 - acc: 0.6850 - val_loss: 0.6190 - val_acc: 0.6734\n",
      "Epoch 3/5\n",
      "2523/2523 [==============================] - 3590s - loss: 0.5906 - acc: 0.6933 - val_loss: 0.6300 - val_acc: 0.6601\n",
      "Epoch 4/5\n",
      "2523/2523 [==============================] - 3611s - loss: 0.5809 - acc: 0.6997 - val_loss: 0.5986 - val_acc: 0.6890\n",
      "Epoch 5/5\n",
      "2523/2523 [==============================] - 3976s - loss: 0.5703 - acc: 0.7098 - val_loss: 0.6109 - val_acc: 0.6805\n"
     ]
    }
   ],
   "source": [
    "top_layer = build_fully_connected_top_layer(base_model.output_shape[1:], dropout=False)\n",
    "top_layer.load_weights(top_layers_weights_path)\n",
    "convx_model = Model(inputs=base_model.input, outputs=top_layer(base_model.output))\n",
    "\n",
    "for layer in convx_model.layers[:len(convx_model.layers) - fine_tuning_layers_to_train]:\n",
    "    layer.trainable = False\n",
    "\n",
    "compile_model(convx_model)\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    target_size=target_image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    VAL_PATH,\n",
    "    target_size=target_image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "fine_tune_history = convx_model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=num_training_steps,\n",
    "    epochs=fine_tuning_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=num_validation_steps,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "convx_model.save_weights(os.path.join(models_save_directory, \"best_model_weights.h5\"))\n",
    "\n",
    "with open(os.path.join(models_save_directory, \"best_model.json\".format(model_name)), \"w\") as json_file:\n",
    "    json_file.write(convx_model.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we will plot the history of our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niBdYV7vzVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17d735d3208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3Xd4VGX2wPHvSQNC7yWFhCZdSujS\nRVERC71IUUF0XdsWXbep+3N1d9V1rYAKAlJEbGDBSpNeBITQQ0kIJXRCCKSc3x/3IjEGkkAmk8yc\nz/PkMTO3nRnDnHnLPa+oKsYYY8zlBHg7AGOMMUWfJQtjjDG5smRhjDEmV5YsjDHG5MqShTHGmFxZ\nsjDGGJMrSxYm30QkSkRURILysO8oEfnhKq61WUS6XenxxpiCYcnCx4nIHhE5LyJVsj2/3v3Aj/JO\nZL9IOsnuzyEReUNEgi/so6pNVHWht2LMSkRqisg7InJARE6LyFYReVpESnsxpnfd/7/JWX425PHY\np0TkPU/HmFfu3+r13o7D5MyShX/YDQy58EBEmgGlvBfOr1RQ1TJAM6AD8BtPXzAvraJs+1cCluO8\nbx1UtSzQC6gA1L3a81+lf6tqmSw/1xbEScVhnxEGsGThL6YBI7I8HglMzbqDiJQXkakikiQie0Xk\nLxc+KEQkUEReEJEjIhIH3JLDsRe+ce8Xkf8TkcD8Bqmqh4FvgMZZzv3zt033m/BsN87TbhdVTJZ9\nnxCRXe62WBG5I8u2USKyVET+KyLHgH+IyDE3cV7Yp5qInBWRqjmE9xhwGhiuqnvceONV9WFV3ege\nryLyGxHZAexwn+soIqtF5KT7347ZYopz490tIsPc5+uJyCL3mCMi8n5+30v3PBdabiNFZJ97rj+7\n23oDTwKDsrZGRGShiDwrIkuBFKCOiNQSkbnu+7VTRMZkucZTIjJHRN53X8c6EbnW3fYHEfkwW0yv\nisjLV/BaxrjXPubGUst9Xtz/p4fd92ujiDR1t93s/h2cdv8uf38l76Nxqar9+PAPsAe4HtgGNAIC\ngXigNqBAlLvfVOBToCwQBWwH7nG3jQO2AhFAJWCBe2yQu/0TYAJQGqgGrALuc7eNAn64RGxR2c5T\nC9gA3J09fvf3p4BU4Gb3dTwHrMiy7wD3HAHAIOAMUDNLHOnAb4EgnBbCG8C/shz/MDDvErGuAJ7O\n5b1WnGRXyT1/JeA4cJd7zSHu48rue3UKuMY9tibQxP19JvBn93WUBK67zDXfBf4vl/f3LTeea4Fz\nQKMs7+d72Y5ZCOwDmrgxBwOL3PeqJNACSAJ6ZjlHGtDf3ff3OC3ZYPc1ncFpOeKe7zDQ+nJ/qzk8\n3wM4ArQCSgCvAovdbTcCa3FaeILzN37h//kBoLP7e0Wglbf/PRbnH2tZ+I8LrYteOB/8+y9scFsB\ng4A/qeppdb45v4jzIQcwEHhZnW/Sx3A+pC8cWx24CXhEVc+o0zr4LzA4H7EdEZETbkxngDmX2fcH\nVf1CVTPc1/Rzl4uqfqCqiaqaqarv43y7b5vl2ERVfVVV01X1LDAFGJqlq+Uu95w5qYzz4ZOb51T1\nmHv+W4AdqjrNveZMnPf+VnffTKCpiJRS1QOqutl9Pg0nmddS1VRVzW2CwO9F5ESWnynZtj+tqmdV\ndQNOMs6tm+pdVd2squlADeA64HE3lvXA21z82wBYq6pzVDUNeAknqbRX1QPAYpwkDtAbOKKqa3O5\nfnbDgEmquk5VzwF/AjqIM96WhvMFpyEgqrrFvS7utsYiUk5Vj6vqunxe12RhycJ/TAOG4nzDnppt\nWxUgBNib5bm9QJj7ey2c1kjWbRfUxvkWeeDChxVOK6NaPmKroqoVgFBgKTD/MvsezPJ7ClDywviA\niIwQZ+D+QhxN3dd2QdbXgKquxElOXUWkIVAPmHuJ6x7F+aacm6zXqMUv3yvcx2GqegYnQY/Dee8+\nd2MA+CPOt+RVblfb3e7re1IuDmKPz3LOF1S1Qpafkdmumf09K5PP13BMVU9nfw057a+qmUCCexw4\nCXm4+/twLp2ML+cX76OqJuP8/whT1e+B14DXgUMiMlFEyrm79sNphe51u/U6XMG1jcuShZ9Q1b04\n3QM3Ax9l23yEi99mL4jkYuvjAE4XVNZtF8TjdG1UyfJhVU5Vm1xBjGdxulU6SLbZW7kRkdo43S0P\nApXd5LMJ50P350vkcOiFD7O7gDmqmnqJS3wL3CG5D/hmvUYiv3xPIcv7qqpfqWovnCS01Y0fVT2o\nqmNUtRZwH/CGiNRT1X/qxUHscbnEkReXKjmd/TVUEpGyOb0G189/G+77E+4eB04XZXN3HKEPMP0K\n4vzF+yjO7LPKXHwfX1HV1jhdZw2AP7jPr1bV23C+uHwCzL6CaxuXJQv/cg/Qw/1W+zO3S2c28KyI\nlHU/eB8DLkyrnA08JCLhIlIReCLLsQeAr4EXRaSciASISF0R6Zrf4ESkBM6H9kGcb475URrnQy7J\nPddonJZFbqYBd+AkjOwtrqxeAsoBU9z3BxEJE5GXRKT5JY75AmggIkNFJEhEBuEM3n8mItVFpK/7\nwXcOSAYy3PMOEJFw9xzH3deVkYfXkl+HgKjLJUBVjQeWAc+JSEn3td7DLz/0W4vInW4L7xGc17PC\nPT4Vp1txBrBKVfflElOwe50LP0HusaNFpIX7N/JPYKWq7hGRNiLSTpzp1mdwxrQyRCRERIaJSHm3\ne+wUnnkP/YYlCz+iqrtUdc0lNv8W5x9bHPADzj/QSe62t4CvcPq71/HrlskInG6sWJwPtznkrcvm\nghMikozz4dUB6Kuq+VpoRVVjccZZlrvnaYbTpZXbcQk4r0mBJZfZ7xjQEacFtlJETgPfASeBnZc4\n5ijOt+nf4SS/PwJ9VPUIzr+93+F8az4GdAUecA9t414jGadb7GFV3X2Zl/FH+eV9Fkdye92uD9z/\nHhWRy/XnD8EZLE8EPgb+rqrfZNn+KU6X2oXB/DvdD+gLpuD8/8hLF9QXwNksP0+p6nfAX4EPcVq5\ndbk4JlYO5+/zOE5X1VHgBXfbXcAeETmF0913oTvMXAHJ579JY3yOiEzCGfz+i7djKW5E5Cmgnqpe\n8oNYRCJxutlqqOqpworNFKzCvHHImCLHnVFzJ9DSu5H4JreL6zFgliWK4s2ShfFbIvIP4FGc6a6X\n6+YxV8AdjzmE0z3U28vhmKtk3VDGGGNyZQPcxhhjcuUz3VBVqlTRqKgob4dhjDHFytq1a4+oak71\n0H7BZ5JFVFQUa9ZcalaoMcaYnIhI9ioDObJuKGOMMbmyZGGMMSZXliyMMcbkymfGLHKSlpZGQkIC\nqamXqg1n8qtkyZKEh4cTHByc+87GGJ/h08kiISGBsmXLEhUVhYjkfoC5LFXl6NGjJCQkEB0d7e1w\njDGFyKPdUCLSW0S2ucshPnGJfQa6Sx9uFpEZ7nO1RWStuzbBZhG5onLMqampVK5c2RJFARERKleu\nbC01Y/yQx1oW7uprr+OszJYArBaRuW510Av71MdZ9aqTqh4XkQsL5hwAOqrqOREpA2xyj00knyxR\nFCx7P43xT55sWbQFdqpqnKqeB2YBt2XbZwzwuqoeB3CX5ERVz7vLJ4Kz5q4NxJviKe0srJsGhzbn\nvq8xRZgnP4TD+OXyjAn8cilGcFa1aiAiS0VkhYj8XGxMRCJEZKN7jn/l1KoQkbEiskZE1iQlJXng\nJVy9EydO8MYbb+T7uJtvvpkTJ054ICJTKFJPwpKX4OVmMPdBeK8/pBzzdlTGXDFPJouc+iuyVy0M\nAuoD3XAWWHlbRCqAs0KXqjbHWRd5pIhU/9XJVCeqaoyqxlStmuvd6l5xqWSRkXH5Rbu++OILKlSo\n4KmwjKecOQLf/QP+2wy+expqXgu3vQ4pR+CTB8AKd5piypOzoRL45brNWdflzbrPCndVrd0isg0n\neay+sIOqJorIZqAzzgpsxcoTTzzBrl27aNGiBcHBwZQpU4aaNWuyfv16YmNjuf3224mPjyc1NZWH\nH36YsWPHAhfLlyQnJ3PTTTdx3XXXsWzZMsLCwvj0008pVaqUl1+Z+YWT+2HZq7D2XUhPhUa3Quff\nQa0WzvbzZ+DLP8LKCdC+IJbPNqZweTJZrAbqi0g0zsLqg4Gh2fb5BKdF8a6IVMHplopz1x8+qqpn\n3TWfO+GsgXzFnp63mdjEgl17pXGtcvz91iaX3ef5559n06ZNrF+/noULF3LLLbewadOmn6eeTpo0\niUqVKnH27FnatGlDv379qFy58i/OsWPHDmbOnMlbb73FwIED+fDDDxk+3FaILBKO7oKlL8P6maCZ\n0HwQXPcIVL3ml/u1HQtxC+Gbv0LtDk6Lw5hixGPJQlXTReRBnLWbA4FJqrpZRJ4B1qjqXHfbDSIS\ni7OY+h9U9aiI9AJeFBHF6c56QVV/8lSshalt27a/uEfhlVde4eOPPwYgPj6eHTt2/CpZREdH06KF\n8w21devW7Nmzp9DiNZdwcBP88BJs/hgCgqH1KOj4W6hYO+f9RZzuqDc7wQej4b5FUKJsoYZszNXw\n6E15qvoFzgLsWZ/7W5bfFWfJxcey7fMN0LwgY8mtBVBYSpcu/fPvCxcu5Ntvv2X58uWEhobSrVu3\nHO9hKFGixM+/BwYGcvbs2UKJ1eQgfhUseRG2z4eQstDxIWj/AJT91ZDar4VWgn5vw5Q+8MUf4I7x\nno/XmALi03dwFwVly5bl9OnTOW47efIkFStWJDQ0lK1bt7JixYpCjs7kiarThbTkRdizBEpVgu5/\ngbb3QqmK+TtXVCfo+jgsfA7qdINrB3sgYGMKniULD6tcuTKdOnWiadOmlCpViurVL34D7d27N+PH\nj6d58+Zcc801tG/f3ouRml/JzIRtXzhJInEdlK0JN/4TWo2EEmWu/Lxd/gC7l8Bnj0FYDFSpV3Ax\nG+MhPrMGd0xMjGZf/GjLli00atTISxH5Lp9/XzPSYdOHzphE0laoGAXXPQrXDoGgErkenicn98P4\nTlA+Au79tuDOa0w+ichaVY3JbT+7M9qYC9JSYfU78Gor+HgsSAD0ewceXOsMYBfkB3r5MLj9TTi4\nEb59quDOa4yHWDeUMeeSYe1kWPYaJB90uoZu+hfUvxECPPh96pqboN04WPEGRHdxHhtTRFmyMP4r\n5Rismggr3oTUExDdFe6c6HxwF1bBxF7PwN5lzt3d9y+FcrUK57rG5JMlC+N/Th+E5a/B6kmQdgau\nuQU6PwbhuXbbFrygEtB/MkzoAh+OgZFzISCw8OMwJheWLIz/OL4Hlv4PfpwOmWnQtL8zcF29sXfj\nqlIPbnkRPhkHi1+Abo97Nx5jcmDJwvi+w1udmU0/zXG+tbcYCp0ehkp1vB3ZRS2GOPdyLHoeojtD\n7Y7ejsiYX7DZUEVMmTLO/P3ExET69++f4z7dunUj+zTh7F5++WVSUlJ+fuyXJc/3r4VZw+CNdrBl\nHrS/Hx7eALf+r2gligtuecGZpvvhvVbO3BQ5liyKqFq1ajFnzpUX2c2eLPym5Lmqc8Pb1NvhrR7O\nHdddH4dHN8ONzxbtAeQSZZ3xi+TD8OlvrJy5KVIsWXjY448//ov1LJ566imefvppevbsSatWrWjW\nrBmffvrpr47bs2cPTZs2BeDs2bMMHjyY5s2bM2jQoF/Uhrr//vuJiYmhSZMm/P3vfwec4oSJiYl0\n796d7t27A07J8yNHjgDw0ksv0bRpU5o2bcrLL7/88/UaNWrEmDFjaNKkCTfccEPxqkGlCtvmwzs3\nOLWXDm12Zho9sgm6P+nUZSoOarWAG/7h3Dm+6i1vR2PMz/xnzOLLJ+BgAReurdEMbnr+srsMHjyY\nRx55hAceeACA2bNnM3/+fB599FHKlSvHkSNHaN++PX379r3k+tZvvvkmoaGhbNy4kY0bN9KqVauf\ntz377LNUqlSJjIwMevbsycaNG3nooYd46aWXWLBgAVWqVPnFudauXcvkyZNZuXIlqkq7du3o2rUr\nFStWLJ6l0DMzIPYTZ1W6Q5ugfCTc/AK0HA7BxXTNj3bjnPGLr/8Mke2hZoHW1DTmiljLwsNatmzJ\n4cOHSUxMZMOGDVSsWJGaNWvy5JNP0rx5c66//nr279/PoUOHLnmOxYsX//yh3bx5c5o3v/jhMXv2\nbFq1akXLli3ZvHkzsbGxl43nhx9+4I477qB06dKUKVOGO++8kyVLlgDFrBR6+nlYNxVei4E5d0PG\nebh9PDy0DtqOKb6JAtxy5m9AaGXntZ1L9nZExvhRyyKXFoAn9e/fnzlz5nDw4EEGDx7M9OnTSUpK\nYu3atQQHBxMVFZVjafKscmp17N69mxdeeIHVq1dTsWJFRo0alet5LlcLrFiUQj9/xkkSy16FU/uh\nZgsYOA0a9vHs3daFrXRluPMtmHKrs8Le7flfx92YguRD/7qKrsGDBzNr1izmzJlD//79OXnyJNWq\nVSM4OJgFCxawd+/eyx7fpUsXpk+fDsCmTZvYuHEjAKdOnaJ06dKUL1+eQ4cO8eWXX/58zKVKo3fp\n0oVPPvmElJQUzpw5w8cff0znzp0L8NV6yNkTsPg/8HIzmP+EM2to+EcwdiE07utbieKC6M7Q9Y+w\nfjpsnO3taIyf85+WhRc1adKE06dPExYWRs2aNRk2bBi33norMTExtGjRgoYNG172+Pvvv5/Ro0fT\nvHlzWrRoQdu2bQG49tpradmyJU2aNKFOnTp06tTp52PGjh3LTTfdRM2aNVmwYMHPz7dq1YpRo0b9\nfI57772Xli1bFt0up+Qkp3bS6rfh3CmofwNc95izNKk/6PJHt5z5oxDWGirX9XZExk9ZiXKTb4Xy\nvp6Id7qa1k2B9HPQ5HYnSfjjYO/JBGc51oq14Z5vrJy5KVB5LVFuLQtTtBzZAT+8DBtnOY+vHQyd\nHvXvBYLKhztjFrOGwrdPQ+9/ejsi44csWZii4cBGZ0W62E8hqCS0uRc6PAgVIrwdWdHQ8BZoOxZW\nvA51ukKDG70dkfEzPp8sVPWS9y+Y/Cvwbst9K5wkseNrKFHOqf7a7n4oU7Vgr+MLev0D9i6HT+6H\ncUuhXE1vR2T8iA9OIbmoZMmSHD16tOA/4PyUqnL06FFKlix5tSeCnd/C5Jth0o1ODacef4VHfoKe\nf7NEcSnBJWHAZGdFv4/GODckGlNIfLplER4eTkJCAklJSZfd73RqGqEhQQQGWAskNyVLliQ8PPzK\nDs7MhK3znJbEgQ1QLgx6/wtajYCQ0IIN1FdVqQ83/wc+fcC5a73rH7wdkfETPp0sgoODiY6Ovuw+\nu5KSGfHKEmqWL8W0e9oSXtE+tApcRppTHvyHl+DIdqhUF/q+Bs0HQVCIt6MrfloMdcqBLPwnRF3n\nP9OIjVd5tBtKRHqLyDYR2SkiT1xin4EiEisim0VkhvtcCxFZ7j63UUQGeSrGulXL8N497TiafI7+\nby5nx6Ff38hmrlDaWacY3iutnIV9AkOg/yR4cDW0ussSxZUSgT4vWTlzU6g8dp+FiAQC24FeQAKw\nGhiiqrFZ9qkPzAZ6qOpxEammqodFpAGgqrpDRGoBa4FGqnrJBRlyus8iP7YcOMWISatIy8jk3dFt\naRHhB+W8PSX1FKyZBMtfhzOHIbwtdPm9c0OdTTYoOIk/wtu9nJlRg96z99ZckbzeZ+HJlkVbYKeq\nxqnqeWAWcFu2fcYAr6vqcQBVPez+d7uq7nB/TwQOAx4d9WxUsxxzxnWgbMkghr61gqU7j3jycr4p\n5Rh8/yy83BS+/TvUaAqjPod7vnY+0OzDrGDVagm9noatnzl3uBvjQZ5MFmFAfJbHCe5zWTUAGojI\nUhFZISK9s59ERNoCIcCuHLaNFZE1IrImt0HsvKhduTRzxnUkomIooyevZv6mA1d9Tr9wKhG++jP8\ntyks/jdEd4ExC+Cuj50+dUsSntP+AafF9tWfC74EvzFZeDJZ5PQJkb3PKwioD3QDhgBvi8jP/T8i\nUhOYBoxW1cxfnUx1oqrGqGpM1aoF0/CoXq4ks+/rQNOwcjwwfR3vr95XIOf1ScfiYN7D8L9rYcWb\n0OhWeGCl0yUS1ir3483VE4Hb34RSFeGD0U5VXmM8wJPJIgHIevttOJCYwz6fqmqaqu4GtuEkD0Sk\nHPA58BdVXeHBOH+lfGgw793bjuvqV+XxD39iwqJfNWr8l6pzt/WH98KrrWH9TGh5l7OOxJ0ToNrl\niyIaDyhdBfq9BUd3OuXMjfEAT06dXQ3UF5FoYD8wGBiabZ9PcFoU74pIFZxuqTgRCQE+Bqaq6gce\njPGSQkOCeHtEDI/NXs9zX27leEoaj/e+xj/vBs/MhMR1sGWe0z9+dCeElHHKcXT4DZSt4e0ITXQX\nZxLB4v9Ane7QrL+3IzI+xmPJQlXTReRB4CsgEJikqptF5BlgjarOdbfdICKxQAbwB1U9KiLDgS5A\nZREZ5Z5ylKqu91S8OQkJCuB/g1tSvlQw4xft4kTKeZ69o5l/3LyXkQZ7fnCSw9bP4fQBCAiCqM7Q\n/n5ocmfxWdfaX3R9wilnPu8RpxuwUh1vR2R8iE+XKC8oqsqLX2/ntQU7ublZDf47qAUlggI9ci2v\nOn8Gdn7nJIft8yH1BASHQr2e0PBWaHCD0zduiq4T8TD+OqgUDXd/bfeymFxZifICJCL8/sZrqBAa\nzP99voXTqWsYP7w1pUv4wNuXcsxJDFs+g13fQ/pZJyFcczM06uN0aVgpjuKjQgTc9hq8Pxy+expu\nfNbbERkf4QOfdoXn3s51KF8qmCc++olhb69k8qg2VCxdDL+5ndzvtB62zoM9S0EznDpNre5y1rKu\n3QkC7U+j2Gp0K7QZA8tfgzrdoH4vb0dkfIB1Q12Brzcf5MGZP1K7UijT7mlHjfJXWYW1MCRtd5LD\nls+cwWqAKg2c5NCoD9RqZfdD+JK0VHi7J5w+CPcvtUkI5pLy2g1lyeIKLdt1hLFT11K+lDPNNrpK\n6UK7dp6oujOYPnMGqY9sd56v1cpJDg1vhaoNvBuj8aykbTCxG4THwF2fQIAPjrOZq2bJohD8lHCS\nkZNXESAw5e62NKlVvlCv/ysZ6bB36cUZTKf2gwRCVCcnOTS8Bcpnv4ne+LR102Dug856IV1+7+1o\nTBFkyaKQ7DyczIh3VnI6NZ13RrWhbXQhTydNO+sMTG/5DLZ/CWePO8uS1u3ptCAa9LYprv5M1bmB\ncvPHMPpLiGzn7YhMEWPJohDtP3GWu95Zyf7jZ3lzeCt6NKzu2QuePQHbv3LGIHZ+B2kpULK8kxga\n9nGmuoYUsW4x4z2pp2BCZ2dlvXFLbPqz+QVLFoXsaPI5Rk1eTeyBU7w44Fpub1nA3T2nDsC2z50W\nxJ4lkJkOZWo4XUuN+jg3ywUGF+w1je/YvxbeuQGuuQkGTrPJDOZndp9FIatcpgQzxrRj7NS1PPL+\nek6knGdUp8uv0pero7sulthIWO1eqJ5TZqNhHwhrDQE+vYy6KShhreH6p+Drv8Cad6DNvd6OyBQz\nliwKUNmSwUwe3YbfzvyRp+bFcuJsGg/3rJ/3elKqztrUWz9zWhBJW5zna7aAHn9xZzBdY98KzZVp\n/xtnOdb5T0JEe2e9EWPyyLqhPCA9I5MnPvqJOWsTGNUxir/1aUzApepJZWbAvuXuFNfP4eQ+kADn\nxriGfZxupgoROR9rTH4lJ8H4TlCyAoxdYGNbxrqhvCkoMIB/92tOhVLBvP3Dbk6knOc/A64lONDt\nMkpLdb7hbZ0H276ElKMQWALq9oBuj0ODm6B0Za++BuOjylSFOyfC1Nth/hPQ91VvR2SKCUsWHhIQ\nIPz5lkZULB3Cf77aRlrKCf7bKomQ7Z/Dzm/hfDKUKOcsN9qwD9S7HkqU8XbYxh/U6QadH4MlL0J0\nVytnbvLEkoUHyZkkflN2CbeHf0DVvSsI2ZdBZulqBDQb4M5g6mJVQY13dPuTU4J+3iPO4Helq5yM\nYXyeJYuCdmz3xQHq+JWAElYxmp31R/Lk1tokB7dkSrf2VC1bwtuRGn8WGAz93nbKmX94D4yeb19c\nzGXZAPfVUoWDP11MEIc3O8/XaObMXmrUB6o1BhEWbU9i3LS1VC9Xgmn3tCOikpX+Nl4W+ynMHgEd\nH4Ib/uHtaIwX2E15npSZAfGr3AQxD07sBQQiO7hF+m6BilE5Hrp27zFGT15NqZBApt3TjgbVyxZO\nzMZcymePOfdeDP/QGTszfsWSRUFLPwdxiy7OYDqTBIEhzmBhwz7OYkFlqubpVFsPnmLEO6s4n5HJ\n5FFtaBlp5ReMF6Wdhbd6QvIhK2fuhyxZFIRzp2HH10730o5v4PxpCCnrLCbTqA/U6wUly13Rqfcd\nTWH4Oys5knyOCXe1pnP9vCUaYzzi8FannHlkOxj+sVUG8COWLK5UchJs+8LpYopbCBnnIbQKNLzZ\nGYOo0xWCCmZw+vCpVEZMWsWupGT+N7glNzerWSDnNeaKrJ0C8x6Cnn93ptYav2DJIj+O780yg2kF\naCZUiLw4QB3RzmMLx5xMSePuKav5cd9xnr2jGUPaRnrkOsbkShXm3O0Met89HyLaejsiUwgsWeTV\nsd3wSgvn92pN3AHqPs5spkKqwZRyPp3731vHou1JPN67Ifd3q1so1zXmV1JPwvjOTuIYtwRKVfB2\nRMbDLFnkx5rJTvdSpToFG1Q+nE/P5HcfbGDehkTu61KHJ25qmPcChMYUpIS1MOkGZ9LGwKlWuNLH\n5TVZ2CgWQMxoryYKgJCgAF4e1ILh7SOZsDiOJz78iYxM30jkppgJbw09/wZb5sLayd6OxuRmz1LY\n/InHL+PRZCEivUVkm4jsFJEnLrHPQBGJFZHNIjIjy/PzReSEiHzmyRiLksAA4R+3NeW3Perx/pp4\nfjN9HefSM7wdlvFHHX7rLM07/09wKNbb0ZicHNwE0wfAuzfD4hecrkMP8liyEJFA4HXgJqAxMERE\nGmfbpz7wJ6CTqjYBHsmy+T/AXZ6Kr6gSEX53wzX8tU9j5m8+yN3vrib5XLq3wzL+JiAA7hjvFLuc\nMxrOp3g7InPB8b3w0VinVEv8SmdRq3u+9nh3oSdbFm2Bnaoap6rngVnAbdn2GQO8rqrHAVT18IUN\nqvodcNqD8RVp91wXzYsDrmUG/Y0ZAAAgAElEQVRF3DGGvbWC42fOezsk42/KVHPKmSdtc8qZG+86\ncwS+fAJebe3MWOv0EDy8Aa57FEI8XzrIk8kiDIjP8jjBfS6rBkADEVkqIitEpHd+LiAiY0VkjYis\nSUpKuspwi55+rcMZP7w1Ww6eZsCE5Rw4edbbIRl/U7c7XPcIrJsCmz7ydjT+6VwyLPwX/K8FrJoA\nLYbAb9dBr2egVOFVf/BkssipTZS9Uy0IqA90A4YAb4tInufqqepEVY1R1ZiqVX3zDuhejasz9e62\nHDyZSv83lxOXlOztkIy/6f5nCG8D8x6G43u8HY3/SD8PKyc6U/sX/hPqdoMHVjoLVpXP/r3b8zyZ\nLBKArOuBhgOJOezzqaqmqepuYBtO8jBZtK9TmVlj25OalsGA8cvZtP+kt0My/iQwGPq9AwjMuQcy\n0rwdkW/LzISNH8DrbeDLP0DVhnDvdzDoPajawGtheTJZrAbqi0i0iIQAg4G52fb5BOgOICJVcLql\n4jwYU7HVNKw8s8d1oGRwIEMmrmBl3FFvh2T8ScXa0PcV2L8Gvv8/b0fjm1Rhx7cwsQt8dK9Th27Y\nhzByHoTnehuEx3ksWahqOvAg8BWwBZitqptF5BkR6evu9hVwVERigQXAH1T1KICILAE+AHqKSIKI\n3OipWIuLulXL8MG4DlQrV4IRk1bxbewhb4dk/EmT26H1aFj6Muz8ztvR+JaENTDlVpjeD1JPwZ1v\nw32Lof71ReamSLuDuxg6duY8oyavYnPiKV4Y0Jw7WoZ7OyTjL9LOwsTukHIExi2FstW9HVHxlrQd\nvn/GWRcntAp0/aOTkAtx1UK7g9uHVSodwowx7WkXXYlH39/A5KW7vR2S8RfBpWDAZGeGzsf3Of3r\nJv9OJcLc38Ib7WHXAmdN9IfXQ7v7iuzytpYsiqkyJYKYNKoNNzSuztPzYnnpm+34SivRFHHVGsFN\nz0PcAlj2irejKV7OHodv/g6vtIT1M6HtGHhoPXR7AkoU7VUzg7wdgLlyJYMDeWNYK/700U+88t0O\nTqSc56lbmxAQUDT6OI0PazXS+Ub8/T+gdieIaOPtiIq2tLOwcgL88JIzJtF8IHR/8pLLLxdFliyK\nuaDAAP7dvzkVQoN5a8luTqSk8eLAawkOtEaj8SARuPV/kLgOPrwb7rNy5jnKSIf102Hh83A60Vld\n8/q/O0sgFDP2ieIDRIQnb27EH3tfw9wNiYyduoaz560AofGwUhWg3yQ4ud+5Yc+6QS9Shdi58GYH\nZ/XBcrVg1OcwfE6xTBRgycJniAgPdKvHP+9oxsLtSYyYtJKTZ+3mKeNhEW2g518h9hOnJIiB3Uvg\n7ethtlsHddB7cO+3EHWdd+O6SpYsfMzQdpG8NqQV6+NPMHjiCg6fTvV2SMbXdXwY6nR3itwd3uLt\naLzn4E/wXj+Y0seZ7dT3Vbh/OTS6tcjcK3E1LFn4oFua1+SdkW3Yc+QMA8YvJ/6YlZc2HhQQAHdM\ngBJl4IPRzmCuPzm+Bz4c4yxHm7DGKfD30DpoNQICfWdY2JKFj+rSoCrTx7TjREoa/d5cxraDflvt\n3RSGstWdhJG0xVkwyR8kJ8EXf4RXY5yb6q57xLlXotPDzv0oPsaShQ9rFVmR2fd1AGDghOWs23fc\nyxEZn1avp/NBuXZyoSzz6TXnTsOC55xqsKvfhpbD4KEfnUWICrFkeGGzZOHjrqlRlg/v70iF0GCG\nvbWSJTt8b90PU4T0+CuExcDch5wV3XxJ+jlYMd5ZV2LR805y/M1KZwpxuZrejs7jLFn4gYhKoXww\nrgO1K4dy97ur+XzjAW+HZHxVYDD0fwdQ+NBHyplnZsKG9+G1GJj/uHMH+73fw8CpUMV/VlSwZOEn\nqpUtyfv3deDa8Ao8OHMdM1bu83ZIxldVjHK+bSeshgX/9HY0V04Vtn8NEzrDx2OhZHkYfqFkeGtv\nR1foLFn4kfKlgpl2Tzu6NajKkx//xBsLd1o9KeMZTe90SoL88F+nLEhxE78a3r0FZgyA88nO4k9j\nF0O9olMyvLBZsvAzpUICmTgihtta1OLf87fx3JdbLWEYz+j9PFS9xqlOm3zY29HkTdI2mDUM3rke\njmyHm1+A36yGZv2dKcJ+zHcmAZs8Cw4M4L8DW1C+VDATF8dxIuU8/7yjGUFWT8oUpJBQ6D8J3uoB\nH4+DYXOK7gfuyf2w8DmnjlNwaWfd8fYPOPeOGMCShd8KCBCe7tuECqEhvPLdDk6eTeN/g1tSMjjQ\n26EZX1K9CfR+Dj57FJa/6kytLUpSjjldZasmgmZCu3HQ+XdQuoq3IytyLFn4MRHhsV4NqBgazNPz\nYrn73dVMHBFDmRL2Z2EKUOvRzrjFd89A7euKxuDw+RRYOd5ZIjb1FDQf5JYMr+3tyIqsItomNIVp\ndKdoXhp4LSt3H2PoWys4dua8t0MyvkQE+r4CZWvBnNGQetJ7sWSkw5rJ8Gor+O5piGgP436AOydY\noshFnpKFiNQVkRLu791E5CERseL1PuTOVuFMGN6abQdPM2D8MhJP+Fl9H+NZpSo691+cTIB5jxR+\nOXNViP0U3mgHnz0C5SNg9JcwbDbUaFq4sRRTeW1ZfAhkiEg94B0gGpjhsaiMV1zfuDpT727L4VPn\nGDB+ObuSkr0dkvElEW2hx59h80fw47TCu+7uxfB2T5g9AgKCYPAMuOdrqN2x8GLwAXlNFpmqmg7c\nAbysqo8Cvn9/ux9qV6cyM8e2JzUtg4Hjl7Npvxe7DIzv6fQo1OnmFOA7vNWz1zqwAabdCVNuhdOH\n4LbX4f5l0PAWv71X4mrkNVmkicgQYCTwmftcsGdCMt7WNKw8H4zrQMngQAZPXMGKuKPeDsn4igvl\nzENKw5y7PVPO/FgczLkHJnRxln294f/gt2uh5XAIsNl+VyqvyWI00AF4VlV3i0g08J7nwjLeVqdq\nGebc34Ea5UsyYtIqvok95O2QjK8oW8NJGIc3w1d/LrjzJh+Gz38Pr7WBrZ87U2AfWg8dfwvBJQvu\nOn4qT8lCVWNV9SFVnSkiFYGyqvp8bseJSG8R2SYiO0XkiUvsM1BEYkVks4jMyPL8SBHZ4f6MzPMr\nMgWmZvlSzL6vA41qlGXce2v5cG2Ct0MyvqL+9dDxIVjzjrNW9dVIPQXfP+tUg10zCVre5ZQM7/k3\nZ51wUyAkL6UeRGQh0Bfnvoz1QBKwSFUfu8wxgcB2oBeQAKwGhqhqbJZ96gOzgR6qelxEqqnqYRGp\nBKwBYgAF1gKtVfWSCzLExMTomjVrcn0tJv+Sz6Vz37Q1LN15lL/2acw910V7OyTjC9LPw6Qb4dgu\nZ/pqhch8Hn8OVr8DS16AlKPQ+HanRHqVep6J10eJyFpVjcltv7x2Q5VX1VPAncBkVW0NXJ/LMW2B\nnaoap6rngVnAbdn2GQO8fiEJqOqFAjI3At+o6jF32zdA7zzGagpYmRJBTBrVht5NavCPz2J58ett\nVk/KXL2gEKcciCp8eK9zD0ReZGbA+pnOCnVf/QmqN4UxC2DgFEsUHpTXZBEkIjWBgVwc4M5NGBCf\n5XGC+1xWDYAGIrJURFaISO98HIuIjBWRNSKyJinJFvXxpBJBgbw2tCWDYiJ49fudDBi/nE9+3E9q\nWoa3QzPFWaVo6PNfiF/p1Ga6HFXY/pWz1vUn4yC0Itz1MYycC2GtCideP5bXug7PAF8BS1V1tYjU\nAXbkckxOc9Oyfx0NAuoD3YBwYImINM3jsajqRGAiON1QucRjrlJQYADP92tGo5plmbxsD4+8v56K\n84Lp3zqcIW0jqVPViq6ZK9CsP8QthCUvQnQXqNP11/vEr4Jv/g77lkHFaKdF0viOoluY0AflKVmo\n6gfAB1kexwH9cjksAYjI8jgcSMxhnxWqmgbsFpFtOMkjASeBZD12YV5iNZ4lIozqFM2IDlEs23WU\n6Sv3MmnpHt5aspuOdSszrF1tejWuTkiQ/SM2+XDTv5yE8NEYGLcUylR1nj+81akpte1zKF0NbnnR\nWScj0GbuF7a8DnCHA68CnXC+4f8APKyql5weIyJBOAPcPYH9OAPcQ1V1c5Z9euMMeo8UkSrAj0AL\nLg5qX2hbrsMZ4D52qevZALf3HD6Vyuw18cxcFc/+E2epUiaEgTERDGkbSUSlUG+HZ4qLQ5thYnen\nddHnv7DwedgwwykZ3ulhaH+/lQz3gLwOcOc1WXyDU97jwj36w4Fhqtorl+NuBl4GAoFJqvqsiDwD\nrFHVuSIiwIs4g9cZOPdxzHKPvRt40j3Vs6o6+XLXsmThfRmZyuLtSUxfuY/vtx5CgS71qzK0XSQ9\nG1az9TJM7la/DZ//DiTAKc3RZoxbMryytyPzWQWdLNaraovcnvMmSxZFS+KJs7y/Op5Zq/dx6NQ5\napQrycA2EQxuE0GtCqW8HZ4pqlRh/hNw/gx0/WP+p9OafCvoZPEt8C4w031qCDBaVXteTZAFyZJF\n0ZSekcn3Ww8zfeU+Fu9IQoAeDasxrF1tujSoSmCA1egxxpsKOllEAq/hlPxQYBnwkKruu9pAC4ol\ni6Iv/lgKM1ftY/aaeI4knyesQimGtI1gYEwE1cpZOQZjvKFAk8UlLvCIqr58RQd7gCWL4uN8eibf\nxB5ixqq9LN15lKAAoVfj6gxtF0mnulUIsNaGMYWmMJLFPlUtMh2KliyKp91HzjBz1T4+WBPP8ZQ0\nalcOZUjbSAa0DqdymRLeDs8Yn1cYySJeVSNy37NwWLIo3lLTMvhq80Gmr9jHqj3HCAkM4MamNRjW\nLpJ20ZUQW3/AGI/Ia7LI6x3cObE7pk2BKRkcyG0twritRRg7Dp1m+sp9fLQugXkbEqlbtTRD29Wm\nX6swKoSGeDtUY/zSZVsWInKanJOCAKVU9WqSTYGyloXvOXs+g882JjJ95T7Wx5+gRFAAtzSvybB2\nkbSKrGitDWMKgMe7oYoaSxa+LTbxFDNW7eWTHxNJPpdOwxplGdoukttbhlGupJV+MOZKWbIwPunM\nuXTmbkhk+sq9bNp/ilLBgfS9thbD2kfSPNwWujEmvyxZGJ+3MeEE01fsY+6GRM6mZdA0rBzD2tWm\n77W1KF2iyPSQGlOkWbIwfuNUahqf/Lif6Sv2se3QacqUCOL2lrUY2rY2jWuV83Z4xhRpliyM31FV\n1u07zvQV+/jspwOcT8+kZWQFhrWrTZ/mNSkZHOjtEI0pcixZGL92IuU8c9YmMGPVPuKSzlCuZBB3\ntgpnWLtI6lcv6+3wjCkyLFkYg9PaWBF3jBmr9jF/0wHSMpS2UZUY1j6S3k1rUCLIWhvGv1myMCab\nI8nnmLM2gZmr9rH3aAoVQ4MZ4C7SFF2ltLfDM8YrLFkYcwmZmcrSXUeYsXIfX8ceIiNT6VTv4pKw\nwbZIk/EjliyMyYNfLwlbgkFtwhncxpaENf7BkoUx+ZCRqSzafpgZK/fx/dbDPy8JO6xdJD1sSVjj\nwyxZGHOFEk+cZdbqeN7PsiTsoDYRDG4bQc3ytiSs8S2WLIy5SukZmXy31WltXFwStjrD2kXakrDG\nZxRGiXJjfFpQYAA3NqnBjU1q/GJJ2G+3HCKsQimGtotkQEw41crakrDG91nLwph8uLAk7PSVe1m2\ny1kS9oYm1RnatjYd61a2JWFNsWMtC2M8IMRdU+OW5jWJS0p2loRdm8AXPx0kyl0Str8tCWt8kLUs\njLlKqWkZzN90kBkrLy4J27tpDYbakrCmGCgSA9wi0hv4HxAIvK2qz2fbPgr4D7Dffeo1VX3b3fYv\n4Bb3+X+o6vuXu5YlC1MUbD90mhkr9/HhugROp6b/vCRs/1bhlA+1RZpM0eP1ZCEigcB2oBeQAKwG\nhqhqbJZ9RgExqvpgtmNvAR4BbgJKAIuAHqp66lLXs2RhipLsS8KWDA7gjpZhjOgQRaOaVjbdFB1F\nYcyiLbBTVePcgGYBtwGxlz3K0RhYpKrpQLqIbAB6A7M9FawxBalUSCADYiIYEBPBpv0neW/FXj7+\ncT8zV8XTNroSIztEcUMTKy1iig9P/qWGAfFZHie4z2XXT0Q2isgcEYlwn9sA3CQioSJSBegORORw\nrDFFXtOw8jzfrzkr/tSTJ29uSOKJs/xmxjqu+9f3vPLdDpJOn/N2iMbkypPJIqdRvex9XvOAKFVt\nDnwLTAFQ1a+BL4BlwExgOZD+qwuIjBWRNSKyJikpqSBjN6bAVQgNYWyXuiz6Q3feGRlDg+pleemb\n7XR8/jsemfUj6/Ydx1cmnBjf48kxiw7AU6p6o/v4TwCq+twl9g8Ejqlq+Ry2zQDeU9UvLnU9G7Mw\nxdGupGSmLd/LnLUJJJ9Lp3l4eUZ0iLKV/UyhKQoD3EE4A9w9cWY7rQaGqurmLPvUVNUD7u93AI+r\nans3cVRQ1aMi0hyYAbRwxzByZMnCFGfJ59L5eF0CU5bvZefhZCqGBjO4bSTD29cmrILVozKe4/Vk\n4QZxM/AyztTZSar6rIg8A6xR1bki8hzQF6eL6Rhwv6puFZGSwDr3NKeAcaq6/nLXsmRhfIGqsnzX\nUaYs38M3sYcA6NW4OiM7RNGhbmW7Z8MUuCKRLAqTJQvjaxKOpzB95T5mrdrH8ZQ06lcrw4iOUdzZ\nMozSJaz4gikYliyM8RGpaRl8tvEAU5bt4af9JylbIoh+rcMZ0aE2daqW8XZ4ppizZGGMj1FVfow/\nwdRle/j8pwOkZShdGlRlZIfadLummpVMN1fEkoUxPuzw6VRmrYpn+sq9HDp1jshKodzVvjYDYsKp\nEBri7fBMMWLJwhg/kJaRydebDzFl+R5W7T5GyeAAbm/hlBVpXMvKipjcWbIwxs/EJp5i2oo9fPzj\nflLTMmkbVYkRHWtzY5MaVlbEXJIlC2P81MmUNGaviWfqij3EHztL9XIlGNauNoPbRtiqfuZXLFkY\n4+cyMpVF2w8zZdleFm1PIjhQuLlZTUZ0iKJVZAW7Z8MARaPqrDHGiwIDhB4Nq9OjYXXikpKZtmIv\nc9Yk8On6RJqGlWNkhyhuvbaWlRUxeWItC2P8yJlz6Xz0436mLtvDDresyKA2kQxvH0l4xVBvh2e8\nwLqhjDGXpKosjzvK1GV7+Tr2IADXN6rOyI5RdLSyIn7FuqGMMZckInSsW4WOdauw/8RZpq/Yy6zV\n8Xwde4h61cowskNt7mgVThkrK2Jc1rIwxgBOWZHPNx5gyvI9bEw4SZkSQfRvHc5dHWpT18qK+Czr\nhjLGXBFVZX38CaYu38tnGxNJy1A616/CyA5RdG9oZUV8jSULY8xVSzp9jlmr9jF95T4OnkolvGIp\n7mpfm0FtIqysiI+wZGGMKTBpGZl8E3uId5c5ZUVKBLllRTrWpkmtXy1uaYoRSxbGGI/YcuAUU5fv\n5ZMf93M2LYM2URUZ0SGK3k2trEhxZMnCGONRJ1PS+GBtPNNW7GXv0RSqlS3B0HaRDG0XaWVFihFL\nFsaYQpGZqSzansSU5XtYuM0pK3JT05qM7FibVpEV7Z6NIs7uszDGFIqAAKF7w2p0b1iN3UfOMG35\nXj5YG8/cDU5ZkREdouhrZUWKPWtZGGMK3Jlz6Xyyfj9Tlu1h+6FkKoQGM6hNBMPb1SaikpUVKUqs\nG8oY43Wqyoq4Y0xdvoevYw+hqvRsVJ2RHaLoVM/KihQF1g1ljPE6EaFD3cp0qFuZxBNnmb5yLzNX\nxfNN7CHqVi3NyI5R3GllRYoFa1kYYwpValoGX/x0gCnL9rDBLSvSr1UYd3WIol41KytS2KwbyhhT\n5K2PP8HUZXv4bOMBzmdk0rl+FUZ0iKKHlRUpNJYsjDHFxpFkp6zIeyucsiKRlUIZ06UOA1qH2ywq\nD8trsvDo7ZYi0ltEtonIThF5Iofto0QkSUTWuz/3Ztn2bxHZLCJbROQVsZEwY3xWlTIleLBHfX54\nvDtvDGtFxdIh/PWTTXR6/nte/W4HJ1LOeztEv+exloWIBALbgV5AArAaGKKqsVn2GQXEqOqD2Y7t\nCPwH6OI+9QPwJ1VdeKnrWcvCGN+hqqzcfYwJi3axYFsSoSGBDG4TyT2dowmrUMrb4fmUojAbqi2w\nU1Xj3IBmAbcBsZc9yqFASSAEECAYOOShOI0xRYyI0L5OZdrXqczWg6eYuCiOqcv3MHX5HvpeW4v7\nutblmhplvR2mX/FkN1QYEJ/lcYL7XHb9RGSjiMwRkQgAVV0OLAAOuD9fqeqW7AeKyFgRWSMia5KS\nkgr+FRhjvK5hjXK8NKgFi/7YnREdopi/+SA3vryY0ZNXsTLuKL4y7lrUeTJZ5DTGkP3/6jwgSlWb\nA98CUwBEpB7QCAjHSTA9RKRLtmNR1YmqGqOqMVWrVi3Q4I0xRUtYhVL87dbGLHuiB7/r1YCNCScZ\nNHEFd7yxjPmbDpCRaUnDkzyZLBKAiCyPw4HErDuo6lFVPec+fAto7f5+B7BCVZNVNRn4EmjvwViN\nMcVEhdAQftuzPkuf6ME/bm/KsTPnGffeOnq9tIiZq/aRmpbh7RB9kieTxWqgvohEi0gIMBiYm3UH\nEamZ5WFf4EJX0z6gq4gEiUgw0DXLNmOMoWRwIHe1r833v+vKa0NbEloikD999BOd/72ANxbu5OTZ\nNG+H6FM8ep+FiNwMvAwEApNU9VkReQZYo6pzReQ5nCSRDhwD7lfVre5MqjdwZkMpMF9VH7vctWw2\nlDH+TVVZtuso4xftYsmOI5QpEcTQdpHc3SmaGuVtfY1LsZvyjDF+a9P+k0xcHMdnGxMJDBBubxHG\nfV3rUK+azaDKzpKFMcbvxR9L4e0lcby/Jp7UtEyub1Sd+7vVoXXtSt4OrciwZGGMMa6jyeeYunwv\nU5bv4URKGjG1KzKua116NKxGgJ/XoLJkYYwx2aScT2f26njeWrKb/SfOUr9aGcZ2qcNtLcIICfJo\n9aMiy5KFMcZcQlpGJl/8dIA3F+5i68HT1ChXknuui2Zw2wjKlgz2dniFypKFMcbkQlVZtD2JCYvi\nWB53lLIlg7irfW1GdYqiWln/mEFlycIYY/JhQ/wJJizexZebDhIcGEC/VuGM7VKH6CqlvR2aR1my\nMMaYK7DnyBkmLoljztoE0jIy6d2kBvd1rUuLiAreDs0jLFkYY8xVSDp9jneX7Wba8r2cSk2nfZ1K\n3Ne1Lt0aVMWXltexZGGMMQUg+Vw6s1bt450fdnPgZCoNa5Tlvq516NO8FsGBxX8GlSULY4wpQOfT\nM5m7IZEJi3ax43AyYRVK/TyDKjTEk0sDeZYlC2OM8YDMTGXBtsNMWBTHqj3HqBAazIj2tRnZMYrK\nZUp4O7x8s2RhjDEetnbvcSYs2sXXsYcoERTAwJgIxnSuQ2TlUG+HlmeWLIwxppDsPJzMW4vj+OjH\nBDIylZub1WRc17o0DSvv7dByZcnCGGMK2aFTqUxaupsZK/Zx+lw619WrwriudelUr3KRnUFlycIY\nY7zkVGoaM1buY9IPuzl8+hxNw8pxX5e63NS0BkFFbAaVJQtjjPGyc+kZfPLjfiYsjiMu6QyRlUIZ\n0zma/q0jKBUS6O3wAEsWxhhTZGRmKt9sOcT4Rbv4cd8JKpUOYVTHKO5qX5uKpUO8GpslC2OMKWJU\nldV7nBlU3209TKngQAa3jeCe66IJr+idGVR5TRbF904SY4wpZkSEttGVaBtdiW0HTzNxcRzTlu9l\n6vK99L22FmO71KFRzXLeDjNH1rIwxhgvSjxxlkk/7Gbmqn2cOZ9Bt2uqcl+XurSvU6lQZlBZN5Qx\nxhQjJ1PSeG/lXiYv3c2R5PNcG1GBcV3qcEOTGgR6cOlXSxbGGFMMpaZl8OG6BN5aHMeeoylEVynN\nmM51uLNVGCWDC34GlSULY4wpxjIyla82H2T8ol1sTDhJlTIlGN0piuHta1O+VMEt/WrJwhhjfICq\nsjzuKOMXxbF4exKlQwIZ2i6Su6+Lpmb5Uld9/rwmC4/eSigivUVkm4jsFJEnctg+SkSSRGS9+3Ov\n+3z3LM+tF5FUEbndk7EaY0xRJCJ0rFuFqXe35YuHOnN94+pMWrqHLv9ewO8/2MCOQ6cLJw5PtSxE\nJBDYDvQCEoDVwBBVjc2yzyggRlUfvMx5KgE7gXBVTbnUftayMMb4i/hjKbzzw25mrd5HalomtzSv\nyWtDWl7R7KmicJ9FW2Cnqsa5Ac0CbgNiL3vUr/UHvrxcojDGGH8SUSmUp/o24aGe9Zm2fC9pGZke\nn2bryWQRBsRneZwAtMthv34i0gWnFfKoqsZn2z4YeCmnC4jIWGAsQGRk5FUHbIwxxUml0iE8fH39\nQrmWJ8csckpz2fu85gFRqtoc+BaY8osTiNQEmgFf5XQBVZ2oqjGqGlO1atUCCNkYY0xOPJksEoCI\nLI/DgcSsO6jqUVU95z58C2id7RwDgY9VNc1jURpjjMmVJ5PFaqC+iESLSAhOd9LcrDu4LYcL+gJb\nsp1jCDDTgzEaY4zJA4+NWahquog8iNOFFAhMUtXNIvIMsEZV5wIPiUhfIB04Boy6cLyIROG0TBZ5\nKkZjjDF5YzflGWOMHysSN+UZY4zxDZYsjDHG5MqShTHGmFz5zJiFiCQBe6/iFFWAIwUUTkGyuPLH\n4sofiyt/fDGu2qqa641qPpMsrpaIrMnLIE9hs7jyx+LKH4srf/w5LuuGMsYYkytLFsYYY3JlyeKi\nid4O4BIsrvyxuPLH4sofv43LxiyMMcbkyloWxhhjcmXJwhhjTK78KlnkYU3wEiLyvrt9pVvMsCjE\nleNa5YUQ1yQROSwimy6xXUTkFTfujSLSqojE1U1ETmZ5v/5WSHFFiMgCEdkiIptF5OEc9in09yyP\ncRX6eyYiJUVklYhscON6Ood9Cv3fZB7j8sq/SffagSLyo4h8lsM2z71fquoXPziVb3cBdYAQYAPQ\nONs+DwDj3d8HA+8XkbhGAa954T3rArQCNl1i+83AlzgLXbUHVhaRuLoBn3nh/aoJ/9/e3YTWUYVh\nHP8/1IsEohYa0WKsXZ895LEAAAR/SURBVNhVBbVIqLqrLgTFLiy04hfiKhsVQUU3bty4EakKolaw\nWgWpH4hUsaSoiFrBUhXRRZEshEhboY3Foja+LuYkuQz3ZibtnTmB+/yg9NyZ08ybNz05dz7uedmU\n2hdQVH8s/yxbz1nNuFrPWcrBaGp3gIPA5lKfHGOyTlxZxmQ69iPAW71+Xk3ma5jOLBZqgkfEP8B8\nTfBuW1ms1rcXuElNF7atF1cWEfEFxdLx/WwFdkfhG2B1qUZJrriyiIiZiDiU2n9S1Ge5rNSt9ZzV\njKt1KQen0stO+lN+4qb1MVkzriwkjQO3Aq/26dJYvoZpsuhVE7w8YBb6RMQZ4CSwZgXEBUWt8h8k\n7ZV0eY/9OdSNPYfr02WEjyVtbPvg6fT/Wop3pd2y5myJuCBDztIllcPAUWB/RPTNV4tjsk5ckGdM\nPgc8BvzXZ39j+RqmyaJOTfA6fQbtnGuVZ5QjX3Ucoljv5mrgeeCDNg8uaRR4F3g4ImbLu3v8k1Zy\nVhFXlpxFxFxEXENRdnlC0lWlLlnyVSOu1sekpNuAoxHx3VLdemwbSL6GabKorAne3UfSecBFNH+5\nYxC1ynOpk9PWRcTs/GWEiNgHdCSNtXFsSR2KX8h7IuK9Hl2y5Kwqrpw5S8c8AXwG3FLalWNMVsaV\naUzeCNwuaZricvUWSW+W+jSWr2GaLCprgqfX96X2NuBApDtFOeNSda3yXD4E7k1P+GwGTkbETO6g\nJF06f51W0gTF//M/WjiugF3AzxHxbJ9ureesTlw5cibpYkmrU3sEuBn4pdSt9TFZJ64cYzIinoiI\n8YhYT/F74kBE3F3q1li+GqvBvdJEvZrgu4A3JB2hmI13rJC4+tYqb5KktymekhmT9BvwFMXNPiLi\nJWAfxdM9R4C/gPtXSFzbgElJZ4DTwI4WJn0o3vndA/yYrncDPAms64otR87qxJUjZ2uB1yWtopic\n3omIj3KPyZpxZRmTvbSVLy/3YWZmlYbpMpSZmZ0lTxZmZlbJk4WZmVXyZGFmZpU8WZiZWSVPFmbL\nIGmua6XRw+qxSvA5fO316rOSrlluQ/M5C7MBOZ2WgTAbKj6zMBsASdOSnkl1EL6VdGXafoWkqbTg\n3JSkdWn7JZLeTwv3fS/phvSlVkl6RUUdhU/TJ4jNsvNkYbY8I6XLUNu79s1GxATwAsXqoKT27rTg\n3B5gZ9q+E/g8Ldy3Cfgpbd8AvBgRG4ETwB0Nfz9mtfgT3GbLIOlURIz22D4NbImIX9Oifb9HxBpJ\nx4G1EfFv2j4TEWOSjgHjXYvRzS8fvj8iNqTXjwOdiHi6+e/MbGk+szAbnOjT7tenl7+72nP4vqKt\nEJ4szAZne9ffX6f2Vywu5nYX8GVqTwGTsFBo58K2gjQ7G37XYrY8I10rtwJ8EhHzj8+eL+kgxZuw\nO9O2B4HXJD0KHGNxldmHgJclPUBxBjEJZF/e3awf37MwG4B0z+K6iDieOxazJvgylJmZVfKZhZmZ\nVfKZhZmZVfJkYWZmlTxZmJlZJU8WZmZWyZOFmZlV+h9PROPBSk6bNgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f24ea2a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_history(fine_tune_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has now been fine tuned and the training process is complete! Lets evaluate the results. While we are also going to look at accuracy, our primary metric of evaluation is going to be f-beta, with a beta score of three. With this, we will get to see which models fit the data well while giving more weight to good recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22424 images belonging to 2 classes.\n",
      "701/701 [==============================] - 992s   \n",
      "Accuracy: 0.6820816981805209\n",
      "F3 Score: 0.7061623655191608\n",
      "Confusion Matrix: \n",
      "[[7937 4193]\n",
      " [2936 7358]]\n"
     ]
    }
   ],
   "source": [
    "convx_model = None\n",
    "with open(os.path.join(models_save_directory, \"best_model.json\"), 'r') as model_file:\n",
    "    convx_model = model_from_json(model_file.read())\n",
    "    \n",
    "convx_model.load_weights(os.path.join(models_save_directory, \"best_model_weights.h5\"))\n",
    "compile_model(convx_model)\n",
    "\n",
    "healthy_test_images = count_files(os.path.join(TEST_PATH, \"healthy\"))\n",
    "unhealthy_test_images = count_files(os.path.join(TEST_PATH, \"unhealthy\"))\n",
    "test_iteration_count = get_iterations_per_epoch((healthy_test_images + unhealthy_test_images), batch_size)\n",
    "\n",
    "# load test data\n",
    "test_labels = np.array(([0] * healthy_test_images) + ([1] * unhealthy_test_images))\n",
    "formatted_test_labels = to_categorical(test_labels, num_classes=2)\n",
    "\n",
    "test_generator = data_generator.flow_from_directory(\n",
    "    TEST_PATH,\n",
    "    target_size=target_image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "raw_predictions = convx_model.predict_generator(test_generator, test_iteration_count, verbose=1)\n",
    "predicted_labels = np.argmax(raw_predictions, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "\n",
    "f3_score = fbeta_score(test_labels, predicted_labels, 3)\n",
    "print(\"F3 Score: {}\".format(f3_score))\n",
    "\n",
    "conf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
    "print(\"Confusion Matrix: \\n{}\".format(conf_matrix))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
